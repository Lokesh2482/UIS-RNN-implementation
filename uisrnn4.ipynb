{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fsspec==2023.6.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:31:33.531810Z",
     "iopub.status.busy": "2025-07-14T15:31:33.531337Z",
     "iopub.status.idle": "2025-07-14T15:31:34.198576Z",
     "shell.execute_reply": "2025-07-14T15:31:34.197791Z",
     "shell.execute_reply.started": "2025-07-14T15:31:33.531784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")  # Replace this with your real token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:31:35.600070Z",
     "iopub.status.busy": "2025-07-14T15:31:35.599805Z",
     "iopub.status.idle": "2025-07-14T15:32:00.230045Z",
     "shell.execute_reply": "2025-07-14T15:32:00.229185Z",
     "shell.execute_reply.started": "2025-07-14T15:31:35.600050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea408cb6514c51911e75b62a09f14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322d7440eb7443d6bc19c8b0ed47396f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00005.parquet:   0%|          | 0.00/446M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7f408aae894ef98259d64521761e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00005.parquet:   0%|          | 0.00/488M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2dadfa18904aa0bd2772fc59d52f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00005.parquet:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba19e7758d24f1193c9e092d76c26f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00005.parquet:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b87631995c42b5bd74309a5ab57502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00005.parquet:   0%|          | 0.00/453M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561c2b1db4743f0849a769bd6eea0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# You can choose from: \"eng\", \"deu\", \"jpn\"\n",
    "ds = load_dataset(\"talkbank/callhome\", \"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:00.231593Z",
     "iopub.status.busy": "2025-07-14T15:32:00.231121Z",
     "iopub.status.idle": "2025-07-14T15:32:05.078078Z",
     "shell.execute_reply": "2025-07-14T15:32:05.077405Z",
     "shell.execute_reply.started": "2025-07-14T15:32:00.231560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from sklearn.cluster import SpectralClustering, KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Set seeds and device\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:09.355395Z",
     "iopub.status.busy": "2025-07-14T15:32:09.354944Z",
     "iopub.status.idle": "2025-07-14T15:32:09.367900Z",
     "shell.execute_reply": "2025-07-14T15:32:09.367157Z",
     "shell.execute_reply.started": "2025-07-14T15:32:09.355373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedFeatureExtractor:\n",
    "    def __init__(self, sr=16000, frame_length=0.025, frame_shift=0.01, target_features=55):\n",
    "        self.sr = sr\n",
    "        self.frame_length = int(frame_length * sr)\n",
    "        self.frame_shift = int(frame_shift * sr)\n",
    "        self.n_mfcc = 13\n",
    "        self.n_fft = 512\n",
    "        self.target_features = target_features\n",
    "        \n",
    "    def extract_features(self, audio_array, original_sr=None):\n",
    "        \"\"\"Extract enhanced features including MFCC, spectral features, and prosodic features\"\"\"\n",
    "        try:\n",
    "            if len(audio_array.shape) > 1:\n",
    "                audio_array = audio_array.mean(axis=1)\n",
    "            \n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if original_sr and original_sr != self.sr:\n",
    "                audio_array = librosa.resample(audio_array, orig_sr=original_sr, target_sr=self.sr)\n",
    "            \n",
    "            # Normalize\n",
    "            audio_array = audio_array / (np.max(np.abs(audio_array)) + 1e-8)\n",
    "            \n",
    "            # Ensure minimum length\n",
    "            min_length = self.sr * 1  # 1 second minimum\n",
    "            if len(audio_array) < min_length:\n",
    "                audio_array = np.pad(audio_array, (0, min_length - len(audio_array)), 'constant')\n",
    "            \n",
    "            # 1. MFCC Features\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=audio_array, sr=self.sr, n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft, hop_length=self.frame_shift,\n",
    "                win_length=self.frame_length, window='hamming'\n",
    "            )\n",
    "            \n",
    "            # 2. Spectral Features\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(\n",
    "                y=audio_array, sr=self.sr, hop_length=self.frame_shift\n",
    "            )\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(\n",
    "                y=audio_array, sr=self.sr, hop_length=self.frame_shift\n",
    "            )\n",
    "            zero_crossing_rate = librosa.feature.zero_crossing_rate(\n",
    "                y=audio_array, hop_length=self.frame_shift\n",
    "            )\n",
    "            \n",
    "            # 3. Prosodic Features (Pitch and Energy)\n",
    "            try:\n",
    "                f0 = librosa.yin(audio_array, fmin=50, fmax=400, sr=self.sr, \n",
    "                               hop_length=self.frame_shift, frame_length=self.frame_length)\n",
    "            except:\n",
    "                # Fallback if yin fails\n",
    "                f0 = np.zeros(mfcc.shape[1])\n",
    "            \n",
    "            # Energy\n",
    "            energy = librosa.feature.rms(y=audio_array, hop_length=self.frame_shift)[0]\n",
    "            \n",
    "            # 4. Delta and Delta-Delta features\n",
    "            delta_mfcc = librosa.feature.delta(mfcc)\n",
    "            delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "            \n",
    "            # 5. Chroma features (for tonal characteristics)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio_array, sr=self.sr, \n",
    "                                               hop_length=self.frame_shift)\n",
    "            \n",
    "            # Combine all features\n",
    "            all_features = [\n",
    "                mfcc,                    # 13 features\n",
    "                delta_mfcc,              # 13 features\n",
    "                delta2_mfcc,             # 13 features\n",
    "                spectral_centroid,       # 1 feature\n",
    "                spectral_rolloff,        # 1 feature\n",
    "                zero_crossing_rate,      # 1 feature\n",
    "                f0.reshape(1, -1),       # 1 feature\n",
    "                energy.reshape(1, -1),   # 1 feature\n",
    "                chroma                   # 12 features\n",
    "            ]\n",
    "            \n",
    "            # Ensure all features have same time dimension\n",
    "            min_time = min(feat.shape[1] for feat in all_features)\n",
    "            all_features = [feat[:, :min_time] for feat in all_features]\n",
    "            \n",
    "            # Stack features\n",
    "            features = np.vstack(all_features)  # Should be 55 features\n",
    "            \n",
    "            # Ensure exact dimension\n",
    "            if features.shape[0] != self.target_features:\n",
    "                # Pad or truncate to target dimension\n",
    "                if features.shape[0] < self.target_features:\n",
    "                    padding = np.zeros((self.target_features - features.shape[0], features.shape[1]))\n",
    "                    features = np.vstack([features, padding])\n",
    "                else:\n",
    "                    features = features[:self.target_features]\n",
    "            \n",
    "            # Normalize each feature dimension\n",
    "            features = (features - np.mean(features, axis=1, keepdims=True)) / (np.std(features, axis=1, keepdims=True) + 1e-8)\n",
    "            \n",
    "            return features.T  # Return as (time, features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Feature extraction error: {e}\")\n",
    "            return np.zeros((100, self.target_features))  # Return dummy features with correct dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:16.900189Z",
     "iopub.status.busy": "2025-07-14T15:32:16.899448Z",
     "iopub.status.idle": "2025-07-14T15:32:16.911438Z",
     "shell.execute_reply": "2025-07-14T15:32:16.910447Z",
     "shell.execute_reply.started": "2025-07-14T15:32:16.900165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdvancedUISRNN(nn.Module):\n",
    "    def __init__(self, input_dim=55, hidden_dim=512, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection and normalization\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.input_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Multi-layer bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim // 2, num_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for better temporal modeling\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim, num_heads=8, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Enhanced speaker embedding head\n",
    "        self.speaker_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 256),  # Final embedding dimension\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "        \n",
    "        # Enhanced change detection head\n",
    "        self.change_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "        # Voice activity detection head\n",
    "        self.vad_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                elif param.dim() >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.normal_(param, mean=0.0, std=0.1)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        # Debug print\n",
    "        if not hasattr(self, '_debug_printed'):\n",
    "            print(f\"Input shape: {x.shape}, Expected input_dim: {self.input_dim}\")\n",
    "            self._debug_printed = True\n",
    "        \n",
    "        # Ensure input dimension matches\n",
    "        if x.size(-1) != self.input_dim:\n",
    "            # Pad or truncate\n",
    "            if x.size(-1) < self.input_dim:\n",
    "                padding = torch.zeros(x.size(0), x.size(1), self.input_dim - x.size(-1), device=x.device)\n",
    "                x = torch.cat([x, padding], dim=-1)\n",
    "            else:\n",
    "                x = x[:, :, :self.input_dim]\n",
    "        \n",
    "        # Input projection and normalization\n",
    "        x = self.input_projection(x)\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        # LSTM processing\n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "            lstm_out, _ = self.lstm(packed)\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        else:\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Self-attention for better temporal modeling\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Residual connection\n",
    "        enhanced_features = lstm_out + attn_out\n",
    "        \n",
    "        # Generate outputs\n",
    "        speaker_embeddings = self.speaker_head(enhanced_features)\n",
    "        change_logits = self.change_head(enhanced_features).squeeze(-1)\n",
    "        vad_logits = self.vad_head(enhanced_features).squeeze(-1)\n",
    "        \n",
    "        return speaker_embeddings, change_logits, vad_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:22.014321Z",
     "iopub.status.busy": "2025-07-14T15:32:22.014004Z",
     "iopub.status.idle": "2025-07-14T15:32:22.035108Z",
     "shell.execute_reply": "2025-07-14T15:32:22.034386Z",
     "shell.execute_reply.started": "2025-07-14T15:32:22.014299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiLanguageDataset(Dataset):\n",
    "    def __init__(self, feature_extractor, max_length=800, min_duration=3.0, \n",
    "                 max_samples_per_lang=None, apply_augmentation=True):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "        self.samples = []\n",
    "        self.language_weights = {}\n",
    "        \n",
    "        print(\"Loading all available languages from CallHome dataset...\")\n",
    "        self._load_all_languages(min_duration, max_samples_per_lang)\n",
    "        \n",
    "    def _load_all_languages(self, min_duration, max_samples_per_lang):\n",
    "        # Available languages in CallHome dataset\n",
    "        languages = ['eng', 'deu', 'jpn', 'ara', 'spa', 'zho', 'yue']\n",
    "        \n",
    "        language_counts = {}\n",
    "        \n",
    "        for lang in languages:\n",
    "            try:\n",
    "                print(f\"Loading {lang} dataset...\")\n",
    "                dataset = load_dataset(\"talkbank/callhome\", lang, trust_remote_code=True)\n",
    "                \n",
    "                # Use train split if available, otherwise use available split\n",
    "                if 'train' in dataset:\n",
    "                    data = dataset['train']\n",
    "                else:\n",
    "                    data = dataset['data'] if 'data' in dataset else dataset[list(dataset.keys())[0]]\n",
    "                \n",
    "                # Limit samples per language if specified\n",
    "                if max_samples_per_lang and len(data) > max_samples_per_lang:\n",
    "                    indices = random.sample(range(len(data)), max_samples_per_lang)\n",
    "                    data = data.select(indices)\n",
    "                \n",
    "                lang_samples = self._process_language_data(data, lang, min_duration)\n",
    "                language_counts[lang] = len(lang_samples)\n",
    "                self.samples.extend(lang_samples)\n",
    "                \n",
    "                print(f\"  {lang}: {len(lang_samples)} valid samples\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {lang}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate language weights for balanced sampling\n",
    "        total_samples = sum(language_counts.values())\n",
    "        for lang, count in language_counts.items():\n",
    "            self.language_weights[lang] = total_samples / (count * len(language_counts))\n",
    "        \n",
    "        print(f\"Total valid samples: {len(self.samples)}\")\n",
    "        print(f\"Language distribution: {language_counts}\")\n",
    "        \n",
    "    def _process_language_data(self, data, language, min_duration):\n",
    "        lang_samples = []\n",
    "        \n",
    "        for idx in tqdm(range(len(data)), desc=f\"Processing {language}\"):\n",
    "            try:\n",
    "                sample = data[idx]\n",
    "                \n",
    "                # Get audio and metadata\n",
    "                audio_data = sample['audio']\n",
    "                audio_array = audio_data['array']\n",
    "                sample_rate = audio_data['sampling_rate']\n",
    "                \n",
    "                # Check duration\n",
    "                duration = len(audio_array) / sample_rate\n",
    "                if duration < min_duration:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.feature_extractor.extract_features(audio_array, sample_rate)\n",
    "                \n",
    "                # Verify feature dimension\n",
    "                if features.shape[1] != self.feature_extractor.target_features:\n",
    "                    print(f\"Warning: Feature dimension mismatch: {features.shape[1]} != {self.feature_extractor.target_features}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get speaker information - create dummy labels if not available\n",
    "                speakers = sample.get('speakers', [])\n",
    "                if not speakers or len(set(speakers)) < 2:\n",
    "                    # Create dummy multi-speaker scenario\n",
    "                    num_frames = len(features)\n",
    "                    mid_point = num_frames // 2\n",
    "                    speakers = [0] * mid_point + [1] * (num_frames - mid_point)\n",
    "                \n",
    "                # Create labels\n",
    "                speaker_labels, change_labels, vad_labels = self._create_labels(speakers, len(features))\n",
    "                \n",
    "                # Limit sequence length\n",
    "                if len(features) > self.max_length:\n",
    "                    features = features[:self.max_length]\n",
    "                    speaker_labels = speaker_labels[:self.max_length]\n",
    "                    change_labels = change_labels[:self.max_length]\n",
    "                    vad_labels = vad_labels[:self.max_length]\n",
    "                \n",
    "                # Apply data augmentation\n",
    "                if self.apply_augmentation:\n",
    "                    aug_samples = self._apply_augmentation(\n",
    "                        features, speaker_labels, change_labels, vad_labels, language\n",
    "                    )\n",
    "                    lang_samples.extend(aug_samples)\n",
    "                else:\n",
    "                    lang_samples.append({\n",
    "                        'features': features,\n",
    "                        'speaker_labels': speaker_labels,\n",
    "                        'change_labels': change_labels,\n",
    "                        'vad_labels': vad_labels,\n",
    "                        'length': len(features),\n",
    "                        'num_speakers': len(set(speakers)),\n",
    "                        'language': language,\n",
    "                        'weight': 1.0\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return lang_samples\n",
    "    \n",
    "    def _create_labels(self, speakers, num_frames):\n",
    "        \"\"\"Create frame-level labels including VAD\"\"\"\n",
    "        if not speakers:\n",
    "            return np.zeros(num_frames), np.zeros(num_frames), np.ones(num_frames)\n",
    "        \n",
    "        # Map speakers to indices\n",
    "        unique_speakers = list(set(speakers))\n",
    "        speaker_to_idx = {spk: i for i, spk in enumerate(unique_speakers)}\n",
    "        \n",
    "        # Create frame-level labels\n",
    "        frames_per_segment = num_frames / len(speakers)\n",
    "        speaker_labels = np.zeros(num_frames, dtype=int)\n",
    "        change_labels = np.zeros(num_frames, dtype=float)\n",
    "        vad_labels = np.ones(num_frames, dtype=float)  # Assume all frames are voiced\n",
    "        \n",
    "        for i in range(num_frames):\n",
    "            segment_idx = min(int(i / frames_per_segment), len(speakers) - 1)\n",
    "            speaker_labels[i] = speaker_to_idx[speakers[segment_idx]]\n",
    "        \n",
    "        # Create change labels with smoothing\n",
    "        change_labels[0] = 1.0\n",
    "        for i in range(1, num_frames):\n",
    "            if speaker_labels[i] != speaker_labels[i-1]:\n",
    "                # Smooth transition\n",
    "                start_idx = max(0, i-2)\n",
    "                end_idx = min(num_frames, i+3)\n",
    "                for j in range(start_idx, end_idx):\n",
    "                    change_labels[j] = max(change_labels[j], \n",
    "                                         np.exp(-0.5 * ((j - i) / 1.0) ** 2))\n",
    "        \n",
    "        return speaker_labels, change_labels, vad_labels\n",
    "    \n",
    "    def _apply_augmentation(self, features, speaker_labels, change_labels, vad_labels, language):\n",
    "        \"\"\"Apply data augmentation techniques\"\"\"\n",
    "        augmented_samples = []\n",
    "        \n",
    "        # Original sample\n",
    "        original_sample = {\n",
    "            'features': features,\n",
    "            'speaker_labels': speaker_labels,\n",
    "            'change_labels': change_labels,\n",
    "            'vad_labels': vad_labels,\n",
    "            'length': len(features),\n",
    "            'num_speakers': len(set(speaker_labels)),\n",
    "            'language': language,\n",
    "            'weight': 1.0\n",
    "        }\n",
    "        augmented_samples.append(original_sample)\n",
    "        \n",
    "        # Augmentation 1: Add noise\n",
    "        noise_level = 0.01\n",
    "        noise = np.random.normal(0, noise_level, features.shape)\n",
    "        noisy_features = features + noise\n",
    "        \n",
    "        augmented_samples.append({\n",
    "            'features': noisy_features,\n",
    "            'speaker_labels': speaker_labels,\n",
    "            'change_labels': change_labels,\n",
    "            'vad_labels': vad_labels,\n",
    "            'length': len(features),\n",
    "            'num_speakers': len(set(speaker_labels)),\n",
    "            'language': language,\n",
    "            'weight': 0.5\n",
    "        })\n",
    "        \n",
    "        # Augmentation 2: Feature masking\n",
    "        if len(features) > 20:\n",
    "            masked_features = features.copy()\n",
    "            mask_length = min(10, len(features) // 10)\n",
    "            mask_start = random.randint(0, len(features) - mask_length)\n",
    "            masked_features[mask_start:mask_start + mask_length] *= 0.1\n",
    "            \n",
    "            augmented_samples.append({\n",
    "                'features': masked_features,\n",
    "                'speaker_labels': speaker_labels,\n",
    "                'change_labels': change_labels,\n",
    "                'vad_labels': vad_labels,\n",
    "                'length': len(features),\n",
    "                'num_speakers': len(set(speaker_labels)),\n",
    "                'language': language,\n",
    "                'weight': 0.3\n",
    "            })\n",
    "        \n",
    "        return augmented_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return {\n",
    "            'features': torch.FloatTensor(sample['features']),\n",
    "            'speaker_labels': torch.LongTensor(sample['speaker_labels']),\n",
    "            'change_labels': torch.FloatTensor(sample['change_labels']),\n",
    "            'vad_labels': torch.FloatTensor(sample['vad_labels']),\n",
    "            'length': torch.tensor(sample['length']),\n",
    "            'num_speakers': torch.tensor(sample['num_speakers']),\n",
    "            'language': sample['language'],\n",
    "            'weight': torch.tensor(sample['weight'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:24.194898Z",
     "iopub.status.busy": "2025-07-14T15:32:24.194164Z",
     "iopub.status.idle": "2025-07-14T15:32:24.199009Z",
     "shell.execute_reply": "2025-07-14T15:32:24.198278Z",
     "shell.execute_reply.started": "2025-07-14T15:32:24.194870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SubsetDataset(Dataset):\n",
    "    \"\"\"Wrapper for dataset subsets that maintains original dataset structure\"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:25.803516Z",
     "iopub.status.busy": "2025-07-14T15:32:25.802921Z",
     "iopub.status.idle": "2025-07-14T15:32:25.830389Z",
     "shell.execute_reply": "2025-07-14T15:32:25.829568Z",
     "shell.execute_reply.started": "2025-07-14T15:32:25.803473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdvancedTrainer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_der = float('inf')\n",
    "        \n",
    "    def train(self, train_loader, val_loader, test_loader, num_epochs=25, \n",
    "              learning_rate=0.0005, patience=5):\n",
    "        \n",
    "        # Advanced optimizer with weight decay\n",
    "        optimizer = optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=1e-4,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "                features = batch['features'].to(self.device)\n",
    "                speaker_labels = batch['speaker_labels'].to(self.device)\n",
    "                change_labels = batch['change_labels'].to(self.device)\n",
    "                vad_labels = batch['vad_labels'].to(self.device)\n",
    "                lengths = batch['length'].to(self.device)\n",
    "                weights = batch['weight'].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                speaker_embeddings, change_logits, vad_logits = self.model(features, lengths)\n",
    "                \n",
    "                # Create mask for valid frames\n",
    "                batch_size, max_len = features.size(0), features.size(1)\n",
    "                mask = torch.arange(max_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "                \n",
    "                # Multi-task loss\n",
    "                loss = self._compute_multi_task_loss(\n",
    "                    speaker_embeddings, change_logits, vad_logits,\n",
    "                    speaker_labels, change_labels, vad_labels,\n",
    "                    mask, weights\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self._validate(val_loader)\n",
    "            \n",
    "            # Test evaluation every 5 epochs\n",
    "            if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "                test_der = self._quick_evaluate(test_loader)\n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, \"\n",
    "                      f\"Val Loss = {val_loss:.4f}, Test DER = {test_der:.4f}\")\n",
    "                \n",
    "                # Save best model based on DER\n",
    "                if test_der < self.best_der:\n",
    "                    self.best_der = test_der\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), 'best_model_der.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f}, \"\n",
    "                      f\"Val Loss = {val_loss:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_model_der.pth'))\n",
    "        return self.model\n",
    "    \n",
    "    def _compute_multi_task_loss(self, speaker_embeddings, change_logits, vad_logits,\n",
    "                                speaker_labels, change_labels, vad_labels, mask, weights):\n",
    "        \"\"\"Compute multi-task loss with proper weighting\"\"\"\n",
    "        \n",
    "        # Get valid predictions\n",
    "        valid_speaker_emb = speaker_embeddings[mask]\n",
    "        valid_change_logits = change_logits[mask]\n",
    "        valid_vad_logits = vad_logits[mask]\n",
    "        valid_speaker_labels = speaker_labels[mask]\n",
    "        valid_change_labels = change_labels[mask]\n",
    "        valid_vad_labels = vad_labels[mask]\n",
    "        \n",
    "        # Expand weights to match valid frames\n",
    "        expanded_weights = weights.unsqueeze(1).expand_as(mask)[mask]\n",
    "        \n",
    "        # 1. Change detection loss\n",
    "        change_loss = F.binary_cross_entropy_with_logits(\n",
    "            valid_change_logits, valid_change_labels,\n",
    "            weight=expanded_weights,\n",
    "            pos_weight=torch.tensor([3.0], device=self.device)\n",
    "        )\n",
    "        \n",
    "        # 2. VAD loss\n",
    "        vad_loss = F.binary_cross_entropy_with_logits(\n",
    "            valid_vad_logits, valid_vad_labels,\n",
    "            weight=expanded_weights\n",
    "        )\n",
    "        \n",
    "        # 3. Speaker embedding loss (enhanced contrastive)\n",
    "        speaker_loss = self._compute_enhanced_speaker_loss(\n",
    "            valid_speaker_emb, valid_speaker_labels, expanded_weights\n",
    "        )\n",
    "        \n",
    "        # 4. Total loss with adaptive weighting\n",
    "        total_loss = (\n",
    "            1.0 * change_loss +\n",
    "            0.2 * vad_loss +\n",
    "            0.3 * speaker_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _compute_enhanced_speaker_loss(self, embeddings, labels, weights):\n",
    "        \"\"\"Enhanced contrastive loss for speaker embeddings\"\"\"\n",
    "        if len(embeddings) < 2:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity = torch.matmul(embeddings, embeddings.t())\n",
    "        \n",
    "        # Create same/different speaker masks\n",
    "        same_speaker = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        different_speaker = 1 - same_speaker\n",
    "        \n",
    "        # Remove diagonal\n",
    "        eye = torch.eye(len(embeddings), device=self.device)\n",
    "        same_speaker = same_speaker * (1 - eye)\n",
    "        different_speaker = different_speaker * (1 - eye)\n",
    "        \n",
    "        # Weighted contrastive loss\n",
    "        weight_matrix = torch.outer(weights, weights)\n",
    "        \n",
    "        positive_loss = same_speaker * weight_matrix * (1 - similarity)\n",
    "        negative_loss = different_speaker * weight_matrix * torch.clamp(similarity - 0.1, min=0)\n",
    "        \n",
    "        total_loss = positive_loss + negative_loss\n",
    "        \n",
    "        return total_loss.sum() / (weight_matrix.sum() + 1e-8)\n",
    "    \n",
    "    def _validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(self.device)\n",
    "                change_labels = batch['change_labels'].to(self.device)\n",
    "                lengths = batch['length'].to(self.device)\n",
    "                weights = batch['weight'].to(self.device)\n",
    "                \n",
    "                _, change_logits, _ = self.model(features, lengths)\n",
    "                \n",
    "                # Create mask\n",
    "                batch_size, max_len = features.size(0), features.size(1)\n",
    "                mask = torch.arange(max_len, device=self.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "                \n",
    "                # Change detection loss\n",
    "                valid_logits = change_logits[mask]\n",
    "                valid_labels = change_labels[mask]\n",
    "                expanded_weights = weights.unsqueeze(1).expand_as(mask)[mask]\n",
    "                \n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    valid_logits, valid_labels, weight=expanded_weights\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        return val_loss / len(val_loader)\n",
    "    \n",
    "    def _quick_evaluate(self, test_loader):\n",
    "        \"\"\"Quick DER evaluation during training\"\"\"\n",
    "        self.model.eval()\n",
    "        ders = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                if len(ders) >= 20:  # Limit for speed\n",
    "                    break\n",
    "                    \n",
    "                features = batch['features'].to(self.device)\n",
    "                speaker_labels = batch['speaker_labels']\n",
    "                lengths = batch['length']\n",
    "                num_speakers = batch['num_speakers']\n",
    "                \n",
    "                speaker_embeddings, change_logits, _ = self.model(features, lengths)\n",
    "                change_probs = torch.sigmoid(change_logits)\n",
    "                \n",
    "                # Process first sample in batch only\n",
    "                length = int(lengths[0])\n",
    "                sample_embeddings = speaker_embeddings[0, :length].cpu().numpy()\n",
    "                sample_change_probs = change_probs[0, :length].cpu().numpy()\n",
    "                sample_speaker_labels = speaker_labels[0, :length].numpy()\n",
    "                sample_num_speakers = int(num_speakers[0])\n",
    "                \n",
    "                # Quick diarization\n",
    "                der = self._quick_diarize(\n",
    "                    sample_embeddings, sample_change_probs, \n",
    "                    sample_speaker_labels, sample_num_speakers\n",
    "                )\n",
    "                ders.append(der)\n",
    "        \n",
    "        return np.mean(ders) if ders else 1.0\n",
    "    \n",
    "    def _quick_diarize(self, embeddings, change_probs, true_labels, num_speakers):\n",
    "        \"\"\"Quick diarization for evaluation during training\"\"\"\n",
    "        # Simple thresholding for change detection\n",
    "        changes = find_peaks(change_probs, height=0.5, distance=5)[0]\n",
    "        if len(changes) == 0:\n",
    "            changes = [0]\n",
    "        \n",
    "        # Create segments\n",
    "        segments = []\n",
    "        start = 0\n",
    "        for change_point in changes:\n",
    "            segments.append((start, change_point))\n",
    "            start = change_point\n",
    "        segments.append((start, len(embeddings)))\n",
    "        \n",
    "        # Assign speakers to segments using clustering\n",
    "        if len(segments) <= num_speakers:\n",
    "            # Simple assignment if few segments\n",
    "            pred_labels = np.zeros(len(embeddings), dtype=int)\n",
    "            for i, (start, end) in enumerate(segments):\n",
    "                pred_labels[start:end] = i % num_speakers\n",
    "        else:\n",
    "            # Cluster segment embeddings\n",
    "            segment_embeddings = []\n",
    "            for start, end in segments:\n",
    "                segment_embeddings.append(np.mean(embeddings[start:end], axis=0))\n",
    "            \n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=num_speakers, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(segment_embeddings)\n",
    "                \n",
    "                pred_labels = np.zeros(len(embeddings), dtype=int)\n",
    "                for i, (start, end) in enumerate(segments):\n",
    "                    pred_labels[start:end] = cluster_labels[i]\n",
    "            except:\n",
    "                # Fallback to simple assignment\n",
    "                pred_labels = np.zeros(len(embeddings), dtype=int)\n",
    "                for i, (start, end) in enumerate(segments):\n",
    "                    pred_labels[start:end] = i % num_speakers\n",
    "        \n",
    "        # Calculate DER\n",
    "        return self._calculate_der(true_labels, pred_labels)\n",
    "    \n",
    "    def _calculate_der(self, true_labels, pred_labels):\n",
    "        \"\"\"Calculate Diarization Error Rate\"\"\"\n",
    "        # Align labels using Hungarian algorithm\n",
    "        true_labels = np.array(true_labels)\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        \n",
    "        # Create mapping from labels to consecutive indices\n",
    "        true_unique = np.unique(true_labels)\n",
    "        pred_unique = np.unique(pred_labels)\n",
    "        \n",
    "        # Create mapping dictionaries\n",
    "        true_map = {label: i for i, label in enumerate(true_unique)}\n",
    "        pred_map = {label: i for i, label in enumerate(pred_unique)}\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        num_true = len(true_unique)\n",
    "        num_pred = len(pred_unique)\n",
    "        confusion = np.zeros((num_true, num_pred))\n",
    "        \n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            if t in true_map and p in pred_map:\n",
    "                confusion[true_map[t], pred_map[p]] += 1\n",
    "        \n",
    "        # Hungarian algorithm for optimal assignment\n",
    "        try:\n",
    "            row_ind, col_ind = linear_sum_assignment(-confusion)\n",
    "            optimal_assignment = dict(zip(col_ind, row_ind))\n",
    "        except:\n",
    "            # Fallback to simple assignment\n",
    "            optimal_assignment = {i: i for i in range(min(num_true, num_pred))}\n",
    "        \n",
    "        # Create mapping from predicted to true labels\n",
    "        pred_to_true = {}\n",
    "        for pred_idx, true_idx in optimal_assignment.items():\n",
    "            pred_label = pred_unique[pred_idx]\n",
    "            true_label = true_unique[true_idx]\n",
    "            pred_to_true[pred_label] = true_label\n",
    "        \n",
    "        # Apply mapping to predicted labels\n",
    "        aligned_pred = np.array([pred_to_true.get(p, -1) for p in pred_labels])\n",
    "        \n",
    "        # Calculate error rate\n",
    "        errors = np.sum(true_labels != aligned_pred)\n",
    "        total = len(true_labels)\n",
    "        \n",
    "        return errors / total if total > 0 else 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:31.065203Z",
     "iopub.status.busy": "2025-07-14T15:32:31.064467Z",
     "iopub.status.idle": "2025-07-14T15:32:31.086371Z",
     "shell.execute_reply": "2025-07-14T15:32:31.085615Z",
     "shell.execute_reply.started": "2025-07-14T15:32:31.065177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdvancedDiarizationPipeline:\n",
    "    def __init__(self, model, feature_extractor, device):\n",
    "        self.model = model.to(device)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.device = device\n",
    "        \n",
    "    def diarize(self, audio_features, num_speakers=None, use_oracle_num_speakers=False):\n",
    "        \"\"\"Complete diarization pipeline with advanced techniques\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Convert to tensor\n",
    "            features_tensor = torch.FloatTensor(audio_features).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Model inference\n",
    "            speaker_embeddings, change_logits, vad_logits = self.model(features_tensor)\n",
    "            \n",
    "            # Get outputs\n",
    "            embeddings = speaker_embeddings.squeeze(0).cpu().numpy()\n",
    "            change_probs = torch.sigmoid(change_logits).squeeze(0).cpu().numpy()\n",
    "            vad_probs = torch.sigmoid(vad_logits).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Step 1: Voice Activity Detection\n",
    "            vad_mask = vad_probs > 0.5\n",
    "            \n",
    "            # Step 2: Change Point Detection with multiple methods\n",
    "            change_points = self._detect_change_points(change_probs, embeddings)\n",
    "            \n",
    "            # Step 3: Segmentation\n",
    "            segments = self._create_segments(change_points, len(embeddings), vad_mask)\n",
    "            \n",
    "            # Step 4: Speaker Number Estimation\n",
    "            if not use_oracle_num_speakers or num_speakers is None:\n",
    "                num_speakers = self._estimate_num_speakers(embeddings, segments)\n",
    "            \n",
    "            # Step 5: Clustering with ensemble method\n",
    "            speaker_labels = self._ensemble_clustering(\n",
    "                embeddings, segments, num_speakers, vad_mask\n",
    "            )\n",
    "            \n",
    "            # Step 6: Post-processing\n",
    "            final_labels = self._post_process_labels(speaker_labels, segments, vad_mask)\n",
    "            \n",
    "        return final_labels, change_points, vad_mask\n",
    "    \n",
    "    def _detect_change_points(self, change_probs, embeddings):\n",
    "        \"\"\"Multi-method change point detection\"\"\"\n",
    "        # Method 1: Probability-based detection\n",
    "        prob_changes = find_peaks(change_probs, height=0.4, distance=10)[0]\n",
    "        \n",
    "        # Method 2: Embedding-based detection using sliding window\n",
    "        window_size = 20\n",
    "        embedding_changes = []\n",
    "        \n",
    "        for i in range(window_size, len(embeddings) - window_size):\n",
    "            left_window = embeddings[i-window_size:i]\n",
    "            right_window = embeddings[i:i+window_size]\n",
    "            \n",
    "            # Calculate cosine distance between windows\n",
    "            left_mean = np.mean(left_window, axis=0)\n",
    "            right_mean = np.mean(right_window, axis=0)\n",
    "            \n",
    "            # Normalize\n",
    "            left_norm = left_mean / (np.linalg.norm(left_mean) + 1e-8)\n",
    "            right_norm = right_mean / (np.linalg.norm(right_mean) + 1e-8)\n",
    "            \n",
    "            # Cosine distance\n",
    "            distance = 1 - np.dot(left_norm, right_norm)\n",
    "            \n",
    "            if distance > 0.3:  # Threshold for change\n",
    "                embedding_changes.append(i)\n",
    "        \n",
    "        # Combine both methods\n",
    "        all_changes = np.unique(np.concatenate([prob_changes, embedding_changes]))\n",
    "        \n",
    "        # Filter changes that are too close\n",
    "        filtered_changes = [all_changes[0]] if len(all_changes) > 0 else []\n",
    "        for change in all_changes[1:]:\n",
    "            if change - filtered_changes[-1] > 15:  # Minimum distance\n",
    "                filtered_changes.append(change)\n",
    "        \n",
    "        return filtered_changes\n",
    "    \n",
    "    def _create_segments(self, change_points, total_length, vad_mask):\n",
    "        \"\"\"Create segments from change points\"\"\"\n",
    "        if not change_points:\n",
    "            return [(0, total_length)]\n",
    "        \n",
    "        segments = []\n",
    "        start = 0\n",
    "        \n",
    "        for change_point in change_points:\n",
    "            if change_point > start:\n",
    "                segments.append((start, change_point))\n",
    "            start = change_point\n",
    "        \n",
    "        # Add final segment\n",
    "        if start < total_length:\n",
    "            segments.append((start, total_length))\n",
    "        \n",
    "        # Filter segments based on VAD and minimum length\n",
    "        filtered_segments = []\n",
    "        for start, end in segments:\n",
    "            # Check if segment has enough voiced frames\n",
    "            segment_vad = vad_mask[start:end]\n",
    "            if np.sum(segment_vad) > 0.3 * (end - start) and (end - start) > 10:\n",
    "                filtered_segments.append((start, end))\n",
    "        \n",
    "        return filtered_segments if filtered_segments else [(0, total_length)]\n",
    "    \n",
    "    def _estimate_num_speakers(self, embeddings, segments):\n",
    "        \"\"\"Estimate number of speakers using multiple methods\"\"\"\n",
    "        # Method 1: Silhouette analysis\n",
    "        segment_embeddings = []\n",
    "        for start, end in segments:\n",
    "            segment_embeddings.append(np.mean(embeddings[start:end], axis=0))\n",
    "        \n",
    "        if len(segment_embeddings) < 2:\n",
    "            return 2\n",
    "        \n",
    "        segment_embeddings = np.array(segment_embeddings)\n",
    "        \n",
    "        best_k = 2\n",
    "        best_score = -1\n",
    "        \n",
    "        for k in range(2, min(8, len(segment_embeddings) + 1)):\n",
    "            try:\n",
    "                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(segment_embeddings)\n",
    "                \n",
    "                if len(set(cluster_labels)) == k:\n",
    "                    score = silhouette_score(segment_embeddings, cluster_labels)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_k = k\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Method 2: Eigengap method\n",
    "        try:\n",
    "            # Compute affinity matrix\n",
    "            distances = pdist(segment_embeddings, metric='cosine')\n",
    "            affinity = np.exp(-squareform(distances))\n",
    "            \n",
    "            # Compute eigenvalues\n",
    "            eigenvalues = np.linalg.eigvals(affinity)\n",
    "            eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "            \n",
    "            # Find eigengap\n",
    "            eigengaps = eigenvalues[:-1] - eigenvalues[1:]\n",
    "            eigengap_k = np.argmax(eigengaps) + 1\n",
    "            \n",
    "            # Combine estimates\n",
    "            estimated_k = int(np.median([best_k, eigengap_k, len(segments) // 2]))\n",
    "            estimated_k = max(2, min(6, estimated_k))\n",
    "            \n",
    "        except:\n",
    "            estimated_k = best_k\n",
    "        \n",
    "        return estimated_k\n",
    "    \n",
    "    def _ensemble_clustering(self, embeddings, segments, num_speakers, vad_mask):\n",
    "        \"\"\"Ensemble clustering with multiple algorithms\"\"\"\n",
    "        # Extract segment embeddings\n",
    "        segment_embeddings = []\n",
    "        segment_info = []\n",
    "        \n",
    "        for start, end in segments:\n",
    "            # Use VAD mask to focus on voiced frames\n",
    "            segment_mask = vad_mask[start:end]\n",
    "            if np.sum(segment_mask) > 0:\n",
    "                voiced_embeddings = embeddings[start:end][segment_mask]\n",
    "                segment_embeddings.append(np.mean(voiced_embeddings, axis=0))\n",
    "            else:\n",
    "                segment_embeddings.append(np.mean(embeddings[start:end], axis=0))\n",
    "            segment_info.append((start, end))\n",
    "        \n",
    "        segment_embeddings = np.array(segment_embeddings)\n",
    "        \n",
    "        # Clustering methods\n",
    "        clustering_results = []\n",
    "        \n",
    "        # Method 1: K-means\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=num_speakers, random_state=42, n_init=10)\n",
    "            kmeans_labels = kmeans.fit_predict(segment_embeddings)\n",
    "            clustering_results.append(kmeans_labels)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Spectral clustering\n",
    "        try:\n",
    "            spectral = SpectralClustering(n_clusters=num_speakers, random_state=42)\n",
    "            spectral_labels = spectral.fit_predict(segment_embeddings)\n",
    "            clustering_results.append(spectral_labels)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 3: Agglomerative clustering\n",
    "        try:\n",
    "            agglomerative = AgglomerativeClustering(n_clusters=num_speakers)\n",
    "            agg_labels = agglomerative.fit_predict(segment_embeddings)\n",
    "            clustering_results.append(agg_labels)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Ensemble voting\n",
    "        if clustering_results:\n",
    "            # Convert to numpy array\n",
    "            clustering_results = np.array(clustering_results)\n",
    "            \n",
    "            # Majority voting for each segment\n",
    "            ensemble_labels = []\n",
    "            for i in range(len(segments)):\n",
    "                votes = clustering_results[:, i]\n",
    "                unique_votes, counts = np.unique(votes, return_counts=True)\n",
    "                majority_label = unique_votes[np.argmax(counts)]\n",
    "                ensemble_labels.append(majority_label)\n",
    "        else:\n",
    "            # Fallback to simple assignment\n",
    "            ensemble_labels = list(range(len(segments)))\n",
    "        \n",
    "        # Convert segment labels to frame labels\n",
    "        frame_labels = np.zeros(len(embeddings), dtype=int)\n",
    "        for i, (start, end) in enumerate(segments):\n",
    "            frame_labels[start:end] = ensemble_labels[i]\n",
    "        \n",
    "        return frame_labels\n",
    "    \n",
    "    def _post_process_labels(self, speaker_labels, segments, vad_mask):\n",
    "        \"\"\"Post-process speaker labels\"\"\"\n",
    "        # Apply VAD mask\n",
    "        processed_labels = speaker_labels.copy()\n",
    "        processed_labels[~vad_mask] = -1  # Mark non-speech as -1\n",
    "        \n",
    "        # Smooth labels to remove very short segments\n",
    "        smoothed_labels = processed_labels.copy()\n",
    "        min_segment_length = 10\n",
    "        \n",
    "        current_speaker = processed_labels[0]\n",
    "        segment_start = 0\n",
    "        \n",
    "        for i in range(1, len(processed_labels)):\n",
    "            if processed_labels[i] != current_speaker:\n",
    "                # Check if previous segment is too short\n",
    "                if i - segment_start < min_segment_length and current_speaker != -1:\n",
    "                    # Merge with neighboring segments\n",
    "                    if segment_start > 0:\n",
    "                        smoothed_labels[segment_start:i] = smoothed_labels[segment_start - 1]\n",
    "                    elif i < len(processed_labels) - 1:\n",
    "                        smoothed_labels[segment_start:i] = processed_labels[i]\n",
    "                \n",
    "                current_speaker = processed_labels[i]\n",
    "                segment_start = i\n",
    "        \n",
    "        return smoothed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:49.656528Z",
     "iopub.status.busy": "2025-07-14T15:32:49.656265Z",
     "iopub.status.idle": "2025-07-14T15:32:49.669190Z",
     "shell.execute_reply": "2025-07-14T15:32:49.668349Z",
     "shell.execute_reply.started": "2025-07-14T15:32:49.656510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedEvaluator:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def evaluate_dataset(self, test_loader, use_oracle_speakers=False):\n",
    "        \"\"\"Comprehensive evaluation on test dataset\"\"\"\n",
    "        all_ders = []\n",
    "        all_language_ders = defaultdict(list)\n",
    "        \n",
    "        print(\"\\nEvaluating on test dataset...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "                features = batch['features'].to(self.pipeline.device)\n",
    "                speaker_labels = batch['speaker_labels']\n",
    "                lengths = batch['length']\n",
    "                num_speakers = batch['num_speakers']\n",
    "                languages = batch['language']\n",
    "                \n",
    "                batch_size = features.size(0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    # Get sample data\n",
    "                    sample_length = int(lengths[i])\n",
    "                    sample_features = features[i, :sample_length].cpu().numpy()\n",
    "                    sample_true_labels = speaker_labels[i, :sample_length].numpy()\n",
    "                    sample_num_speakers = int(num_speakers[i])\n",
    "                    sample_language = languages[i] if isinstance(languages, list) else languages\n",
    "                    \n",
    "                    # Diarization\n",
    "                    pred_labels, change_points, vad_mask = self.pipeline.diarize(\n",
    "                        sample_features, \n",
    "                        num_speakers=sample_num_speakers if use_oracle_speakers else None,\n",
    "                        use_oracle_num_speakers=use_oracle_speakers\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate DER\n",
    "                    der = self._calculate_detailed_der(sample_true_labels, pred_labels, vad_mask)\n",
    "                    \n",
    "                    all_ders.append(der)\n",
    "                    all_language_ders[sample_language].append(der)\n",
    "                    \n",
    "                    # Print progress every 50 samples\n",
    "                    if len(all_ders) % 50 == 0:\n",
    "                        print(f\"Processed {len(all_ders)} samples, Average DER: {np.mean(all_ders):.3f}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        overall_der = np.mean(all_ders)\n",
    "        overall_std = np.std(all_ders)\n",
    "        \n",
    "        print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(f\"Overall DER: {overall_der:.3f}  {overall_std:.3f}\")\n",
    "        print(f\"Median DER: {np.median(all_ders):.3f}\")\n",
    "        print(f\"Best DER: {np.min(all_ders):.3f}\")\n",
    "        print(f\"Worst DER: {np.max(all_ders):.3f}\")\n",
    "        \n",
    "        print(f\"\\n=== LANGUAGE-SPECIFIC RESULTS ===\")\n",
    "        for lang, ders in all_language_ders.items():\n",
    "            lang_mean = np.mean(ders)\n",
    "            lang_std = np.std(ders)\n",
    "            print(f\"{lang}: {lang_mean:.3f}  {lang_std:.3f} ({len(ders)} samples)\")\n",
    "        \n",
    "        return overall_der, all_ders, dict(all_language_ders)\n",
    "    \n",
    "    def _calculate_detailed_der(self, true_labels, pred_labels, vad_mask):\n",
    "        \"\"\"Calculate detailed DER with proper handling of non-speech frames\"\"\"\n",
    "        # Only consider voiced frames\n",
    "        voiced_indices = np.where(vad_mask)[0]\n",
    "        \n",
    "        if len(voiced_indices) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        voiced_true = true_labels[voiced_indices]\n",
    "        voiced_pred = pred_labels[voiced_indices]\n",
    "        \n",
    "        # Handle -1 labels (non-speech) in predictions\n",
    "        speech_indices = voiced_pred != -1\n",
    "        if np.sum(speech_indices) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        speech_true = voiced_true[speech_indices]\n",
    "        speech_pred = voiced_pred[speech_indices]\n",
    "        \n",
    "        # Align labels using Hungarian algorithm\n",
    "        return self._align_and_calculate_der(speech_true, speech_pred)\n",
    "    \n",
    "    def _align_and_calculate_der(self, true_labels, pred_labels):\n",
    "        \"\"\"Align predicted labels with true labels and calculate DER\"\"\"\n",
    "        if len(true_labels) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        # Get unique labels\n",
    "        true_speakers = list(set(true_labels))\n",
    "        pred_speakers = list(set(pred_labels))\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        confusion = np.zeros((len(true_speakers), len(pred_speakers)))\n",
    "        \n",
    "        for i, true_spk in enumerate(true_speakers):\n",
    "            for j, pred_spk in enumerate(pred_speakers):\n",
    "                confusion[i, j] = np.sum((true_labels == true_spk) & (pred_labels == pred_spk))\n",
    "        \n",
    "        # Hungarian algorithm for optimal assignment\n",
    "        try:\n",
    "            row_ind, col_ind = linear_sum_assignment(-confusion)\n",
    "            \n",
    "            # Create mapping\n",
    "            mapping = {}\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                mapping[pred_speakers[c]] = true_speakers[r]\n",
    "            \n",
    "            # Apply mapping\n",
    "            aligned_pred = np.array([mapping.get(p, p) for p in pred_labels])\n",
    "            \n",
    "            # Calculate error rate\n",
    "            errors = np.sum(true_labels != aligned_pred)\n",
    "            total = len(true_labels)\n",
    "            \n",
    "            return errors / total\n",
    "            \n",
    "        except:\n",
    "            # Fallback: simple accuracy\n",
    "            return 1.0 - (np.sum(true_labels == pred_labels) / len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:32:56.240683Z",
     "iopub.status.busy": "2025-07-14T15:32:56.239973Z",
     "iopub.status.idle": "2025-07-14T18:02:25.463969Z",
     "shell.execute_reply": "2025-07-14T18:02:25.462405Z",
     "shell.execute_reply.started": "2025-07-14T15:32:56.240658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Multi-Language UIS-RNN for CallHome Dataset ===\n",
      "Target: Sub-20% DER with comprehensive techniques\n",
      "Device: cuda\n",
      "\n",
      "1. Loading multi-language dataset...\n",
      "Loading all available languages from CallHome dataset...\n",
      "Loading eng dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing eng: 100%|| 140/140 [25:15<00:00, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eng: 420 valid samples\n",
      "Loading deu dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1e2cba36424a399ec15ec5dda63de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00005.parquet:   0%|          | 0.00/389M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9d2f6c1a6443a582908a600cf0769a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00005.parquet:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45b96a69dfc47c98c7021eedeebdde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00005.parquet:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c124138e524a918f605a5dcc1c7d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00005.parquet:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef3fac7f3f94777a5ed46738c9ce167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00005.parquet:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07a360d72d646a0af1449080346c296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing deu: 100%|| 120/120 [22:39<00:00, 11.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  deu: 360 valid samples\n",
      "Loading jpn dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf52dae2d3f4ff989a4e054723fb835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00005.parquet:   0%|          | 0.00/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51843645d8cf4ac18b5450c3d5301355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00005.parquet:   0%|          | 0.00/403M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e0766d527e4e3c9d71b21ed7acfa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00005.parquet:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7dd0010ae491590d06ee4ffc0c1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00005.parquet:   0%|          | 0.00/402M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e6b11ea7c0430aae0946791c703dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00005.parquet:   0%|          | 0.00/414M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b0021a75904930a372a5e2c3b9be33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing jpn: 100%|| 120/120 [22:57<00:00, 11.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  jpn: 360 valid samples\n",
      "Loading ara dataset...\n",
      "  Error loading ara: BuilderConfig 'ara' not found. Available: ['deu', 'eng', 'jpn', 'spa', 'zho']\n",
      "Loading spa dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074e3e846bd84299ae1fbeb7aba65437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00005.parquet:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3513ef74e54b53b6897e93bc60dbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00005.parquet:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5382131d0fea4a06a79fb1bb7376b06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00005.parquet:   0%|          | 0.00/500M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb87b09d7026464381f7ebe7564fc13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00005.parquet:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b9cf4302ca40f1893629ed4b384fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00005.parquet:   0%|          | 0.00/459M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491bd357043f488faafd7f394649208d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spa: 100%|| 140/140 [26:36<00:00, 11.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spa: 420 valid samples\n",
      "Loading zho dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4bf819278845a5876cfe1e906709d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00000-of-00005.parquet:   0%|          | 0.00/449M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f91a00627624c2da7729c1862745c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00001-of-00005.parquet:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0510faa7b74348a68082463d0798ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00002-of-00005.parquet:   0%|          | 0.00/477M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccc5f2fc14e4bf9bbbaf3411693c966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00003-of-00005.parquet:   0%|          | 0.00/468M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b4d3af2fe24a28b2c2f72fcedc5d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data-00004-of-00005.parquet:   0%|          | 0.00/428M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6764702f04a420fb255d58212f3e2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split:   0%|          | 0/140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing zho: 100%|| 140/140 [25:16<00:00, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  zho: 420 valid samples\n",
      "Loading yue dataset...\n",
      "  Error loading yue: BuilderConfig 'yue' not found. Available: ['deu', 'eng', 'jpn', 'spa', 'zho']\n",
      "Total valid samples: 1980\n",
      "Language distribution: {'eng': 420, 'deu': 360, 'jpn': 360, 'spa': 420, 'zho': 420}\n",
      "Dataset splits: Train=1386, Val=297, Test=297\n",
      "\n",
      "2. Initializing Advanced UIS-RNN model...\n",
      "Model parameters: 6,501,634\n",
      "\n",
      "3. Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 600, 55]), Expected input_dim: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 174/174 [00:27<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.3055, Val Loss = 0.0386, Test DER = 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 174/174 [00:26<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.2240, Val Loss = 0.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 174/174 [00:26<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.2181, Val Loss = 0.0343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 174/174 [00:26<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.2165, Val Loss = 0.0337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 174/174 [00:26<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.2161, Val Loss = 0.0346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 174/174 [00:26<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.2115, Val Loss = 0.0316, Test DER = 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 174/174 [00:27<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.2136, Val Loss = 0.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 174/174 [00:27<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.2121, Val Loss = 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 174/174 [00:27<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.2112, Val Loss = 0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 174/174 [00:27<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.2107, Val Loss = 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 174/174 [00:27<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 0.2097, Val Loss = 0.0332, Test DER = 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 174/174 [00:27<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 0.2104, Val Loss = 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 174/174 [00:27<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 0.2081, Val Loss = 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 174/174 [00:27<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 0.2067, Val Loss = 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 174/174 [00:27<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 0.2076, Val Loss = 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|| 174/174 [00:27<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 0.2077, Val Loss = 0.0324, Test DER = 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|| 174/174 [00:27<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 0.2077, Val Loss = 0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|| 174/174 [00:27<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 0.2025, Val Loss = 0.0331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|| 174/174 [00:27<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 0.2042, Val Loss = 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|| 174/174 [00:27<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 0.2039, Val Loss = 0.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|| 174/174 [00:27<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss = 0.2012, Val Loss = 0.0328, Test DER = 0.4146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|| 174/174 [00:27<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss = 0.1988, Val Loss = 0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|| 174/174 [00:27<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss = 0.1949, Val Loss = 0.0395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|| 174/174 [00:27<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss = 0.1960, Val Loss = 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|| 174/174 [00:27<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss = 0.1908, Val Loss = 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|| 174/174 [00:27<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss = 0.1899, Val Loss = 0.0324, Test DER = 0.4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|| 174/174 [00:27<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss = 0.1855, Val Loss = 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|| 174/174 [00:27<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss = 0.1851, Val Loss = 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|| 174/174 [00:27<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss = 0.1824, Val Loss = 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|| 174/174 [00:27<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss = 0.1788, Val Loss = 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|| 174/174 [00:27<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss = 0.1781, Val Loss = 0.0319, Test DER = 0.3510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|| 174/174 [00:27<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss = 0.1746, Val Loss = 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|| 174/174 [00:27<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss = 0.1745, Val Loss = 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|| 174/174 [00:27<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss = 0.1785, Val Loss = 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|| 174/174 [00:27<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss = 0.1762, Val Loss = 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|| 174/174 [00:27<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss = 0.1821, Val Loss = 0.0311, Test DER = 0.3967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|| 174/174 [00:27<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss = 0.1813, Val Loss = 0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|| 174/174 [00:27<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss = 0.1773, Val Loss = 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|| 174/174 [00:27<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss = 0.1741, Val Loss = 0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|| 174/174 [00:27<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Train Loss = 0.1711, Val Loss = 0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|| 174/174 [00:27<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Train Loss = 0.1649, Val Loss = 0.0295, Test DER = 0.3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|| 174/174 [00:27<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Train Loss = 0.1662, Val Loss = 0.0317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|| 174/174 [00:27<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Train Loss = 0.1621, Val Loss = 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|| 174/174 [00:27<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Train Loss = 0.1603, Val Loss = 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|| 174/174 [00:27<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Train Loss = 0.1559, Val Loss = 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|| 174/174 [00:27<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Train Loss = 0.1557, Val Loss = 0.0247, Test DER = 0.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|| 174/174 [00:27<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Train Loss = 0.1514, Val Loss = 0.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|| 174/174 [00:27<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Train Loss = 0.1494, Val Loss = 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|| 174/174 [00:27<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Train Loss = 0.1470, Val Loss = 0.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|| 174/174 [00:27<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Train Loss = 0.1446, Val Loss = 0.0230, Test DER = 0.2407\n",
      "\n",
      "4. Creating diarization pipeline...\n",
      "\n",
      "5. Final evaluation...\n",
      "\n",
      "--- Evaluation without oracle speaker count ---\n",
      "\n",
      "Evaluating on test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82/1671589827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_82/1671589827.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Test without oracle number of speakers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Evaluation without oracle speaker count ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mfinal_der\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_ders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_oracle_speakers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# Test with oracle number of speakers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_82/906658654.py\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(self, test_loader, use_oracle_speakers)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Diarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                     pred_labels, change_points, vad_mask = self.pipeline.diarize(\n\u001b[0m\u001b[1;32m     32\u001b[0m                         \u001b[0msample_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mnum_speakers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_num_speakers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_oracle_speakers\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_82/1794955938.py\u001b[0m in \u001b[0;36mdiarize\u001b[0;34m(self, audio_features, num_speakers, use_oracle_num_speakers)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Step 3: Segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_segments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvad_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Step 4: Speaker Number Estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_82/1794955938.py\u001b[0m in \u001b[0;36m_create_segments\u001b[0;34m(self, change_points, total_length, vad_mask)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Check if segment has enough voiced frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0msegment_vad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvad_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegment_vad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mfiltered_segments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "def create_weighted_sampler(dataset):\n",
    "    \"\"\"Create weighted sampler for balanced training\"\"\"\n",
    "    # Calculate sample weights based on language and augmentation\n",
    "    weights = []\n",
    "    language_counts = defaultdict(int)\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # First pass: count samples per language\n",
    "    for idx in range(total_samples):\n",
    "        sample = dataset[idx]\n",
    "        language_counts[sample['language']] += 1\n",
    "    \n",
    "    # Second pass: calculate weights\n",
    "    for idx in range(total_samples):\n",
    "        sample = dataset[idx]\n",
    "        lang = sample['language']\n",
    "        # Inverse frequency weighting\n",
    "        lang_weight = total_samples / (language_counts[lang] * len(language_counts))\n",
    "        # Augmentation weight\n",
    "        aug_weight = sample['weight']\n",
    "        # Combined weight\n",
    "        final_weight = lang_weight * aug_weight\n",
    "        weights.append(final_weight)\n",
    "    \n",
    "    return WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences\"\"\"\n",
    "    # Sort by length (descending) for better packing\n",
    "    batch.sort(key=lambda x: x['length'], reverse=True)\n",
    "    \n",
    "    # Get maximum length\n",
    "    max_length = batch[0]['length']\n",
    "    batch_size = len(batch)\n",
    "    feature_dim = batch[0]['features'].size(1)\n",
    "    \n",
    "    # Initialize tensors\n",
    "    features = torch.zeros(batch_size, max_length, feature_dim)\n",
    "    speaker_labels = torch.zeros(batch_size, max_length, dtype=torch.long)\n",
    "    change_labels = torch.zeros(batch_size, max_length)\n",
    "    vad_labels = torch.zeros(batch_size, max_length)\n",
    "    lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    num_speakers = torch.zeros(batch_size, dtype=torch.long)\n",
    "    weights = torch.zeros(batch_size)\n",
    "    languages = []\n",
    "    \n",
    "    # Fill tensors\n",
    "    for i, sample in enumerate(batch):\n",
    "        length = sample['length']\n",
    "        features[i, :length] = sample['features']\n",
    "        speaker_labels[i, :length] = sample['speaker_labels']\n",
    "        change_labels[i, :length] = sample['change_labels']\n",
    "        vad_labels[i, :length] = sample['vad_labels']\n",
    "        lengths[i] = length\n",
    "        num_speakers[i] = sample['num_speakers']\n",
    "        weights[i] = sample['weight']\n",
    "        languages.append(sample['language'])\n",
    "    \n",
    "    return {\n",
    "        'features': features,\n",
    "        'speaker_labels': speaker_labels,\n",
    "        'change_labels': change_labels,\n",
    "        'vad_labels': vad_labels,\n",
    "        'length': lengths,\n",
    "        'num_speakers': num_speakers,\n",
    "        'weight': weights,\n",
    "        'language': languages\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=== Enhanced Multi-Language UIS-RNN for CallHome Dataset ===\")\n",
    "    print(\"Target: Sub-20% DER with comprehensive techniques\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Initialize feature extractor\n",
    "    feature_extractor = EnhancedFeatureExtractor()\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"\\n1. Loading multi-language dataset...\")\n",
    "    dataset = MultiLanguageDataset(\n",
    "        feature_extractor=feature_extractor,\n",
    "        max_length=600,\n",
    "        min_duration=2.0,\n",
    "        max_samples_per_lang=500,  # Limit for faster training\n",
    "        apply_augmentation=True\n",
    "    )\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"ERROR: No valid samples found!\")\n",
    "        return\n",
    "\n",
    "    # Create train/validation/test splits\n",
    "    total_samples = len(dataset)\n",
    "    train_size = int(0.7 * total_samples)\n",
    "    val_size = int(0.15 * total_samples)\n",
    "    test_size = total_samples - train_size - val_size\n",
    "\n",
    "    # Random split\n",
    "    indices = list(range(total_samples))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    # Create dataset subsets\n",
    "    train_dataset = SubsetDataset(dataset, train_indices)\n",
    "    val_dataset = SubsetDataset(dataset, val_indices)\n",
    "    test_dataset = SubsetDataset(dataset, test_indices)\n",
    "\n",
    "    print(f\"Dataset splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_sampler = create_weighted_sampler(train_dataset)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=8, sampler=train_sampler,\n",
    "        collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=8, shuffle=False,\n",
    "        collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=4, shuffle=False,\n",
    "        collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\n2. Initializing Advanced UIS-RNN model...\")\n",
    "    model = AdvancedUISRNN(\n",
    "        input_dim=feature_extractor.target_features,\n",
    "        hidden_dim=512,\n",
    "        num_layers=3,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Training\n",
    "    print(\"\\n3. Training model...\")\n",
    "    trainer = AdvancedTrainer(model, device)\n",
    "    trained_model = trainer.train(\n",
    "        train_loader, val_loader, test_loader,\n",
    "        num_epochs=50, learning_rate=0.0001, patience=10\n",
    "    )\n",
    "\n",
    "    # Create diarization pipeline\n",
    "    print(\"\\n4. Creating diarization pipeline...\")\n",
    "    pipeline = AdvancedDiarizationPipeline(trained_model, feature_extractor, device)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\n5. Final evaluation...\")\n",
    "    evaluator = EnhancedEvaluator(pipeline)\n",
    "    \n",
    "    # Test without oracle number of speakers\n",
    "    print(\"\\n--- Evaluation without oracle speaker count ---\")\n",
    "    final_der, all_ders, lang_ders = evaluator.evaluate_dataset(test_loader, use_oracle_speakers=False)\n",
    "    \n",
    "    # Test with oracle number of speakers\n",
    "    print(\"\\n--- Evaluation with oracle speaker count ---\")\n",
    "    oracle_der, oracle_ders, oracle_lang_ders = evaluator.evaluate_dataset(test_loader, use_oracle_speakers=True)\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Final DER (estimated speakers): {final_der:.3f}\")\n",
    "    print(f\"Final DER (oracle speakers): {oracle_der:.3f}\")\n",
    "    print(f\"Target achieved: {'' if final_der < 0.20 else ''} (Target: <20%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
